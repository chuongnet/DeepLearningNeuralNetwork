{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "624dc057",
   "metadata": {},
   "source": [
    "# Stacked generalization ensemble\n",
    "In stacking, an algorithm takes the outputs of sub-models as input and attempts to learn how to best combine the input predictions to make a better output prediction.\n",
    "\n",
    "It may be helpful to think of the stacking procedure as having two levels: level 0 and level 1.\n",
    "\n",
    "- Level 0: The level 0 data is the training dataset inputs and level 0 models learn to make predictions from this data.\n",
    "- Level 1: The level 1 data takes the output of the level 0 models as input and the single level 1 model, or meta-learner, learns to make predictions from this data.\n",
    "\n",
    "## Train and save sub-models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81432ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "import matplotlib.pyplot as plt\n",
    "from os import makedirs, listdir\n",
    "from os.path import exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87ccbd49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create datasets\n",
    "def prepare_data():\n",
    "    X, y = make_blobs(n_samples=1100, centers=3, cluster_std=2, random_state=2)\n",
    "    #y = to_categorical(y)\n",
    "    # split data\n",
    "    n_data = 100\n",
    "    trainX, testX = X[:n_data, :], X[n_data:, :]\n",
    "    trainy, testy = y[:n_data], y[n_data:]\n",
    "    return trainX, trainy, testX, testy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3ff41d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create keras models and saving sub-models \n",
    "def fit_model(trainX, trainy):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(25, input_dim=2, activation='relu'))\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.fit(trainX, trainy, epochs=500, verbose=0)\n",
    "    return model\n",
    "\n",
    "# build data\n",
    "trainX, trainy, testX, testy = prepare_data()\n",
    "# create models and save to disk.\n",
    "if not exists(\"models\"):\n",
    "    makedirs('models')\n",
    "    n_models = 5\n",
    "    for i in range(n_models):\n",
    "        model = fit_model(trainX, trainy)\n",
    "        filename = 'models/model_' + str(i) + '.h5'\n",
    "        model.save(filename)\n",
    "        print(\"saved %s\" % filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf712e7",
   "metadata": {},
   "source": [
    "# Stacking model, dataset and prediction sub-model\n",
    "As input for a new model, we will require 1,000 examples with some number of features. Given that we have five models and each model makes three predictions per example, then we would have 15 (3 x 5) features for each example provided to the submodels. We can transform the [1000, 5, 3] shaped predictions from the sub-models into a [1000, 15] shaped array to be used to train a meta-learner using the reshape() NumPy function and flattening the final two dimensions. The stacked_dataset() function implements this step.\n",
    "\n",
    "we can use this input dataset along with the output, or y part, of the test set to train a new meta-learner.\n",
    "\n",
    "In this case, we will train a simple logistic regression algorithm from the scikit-learn library.\n",
    "\n",
    "Logistic regression only supports binary classification, although the implementation of logistic regression in scikit-learn in the LogisticRegression class supports multi-class classification (more than two classes) using a one-vs-rest scheme. The function fit_stacked_model() below will prepare the training dataset for the meta-learner by calling the stacked_dataset() function, then fit a logistic regression model that is then returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38403495",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from tensorflow.keras.models import load_model\n",
    "from numpy import dstack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76258a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load models\n",
    "def load_models(n_models):\n",
    "    all_models = list()\n",
    "    for i in range(n_models):\n",
    "        filename = 'models/model_' + str(i) + '.h5'\n",
    "        model = load_model(filename)\n",
    "        all_models.append(model)\n",
    "        print('loaded %s' %  filename)\n",
    "    return all_models\n",
    "\n",
    "# create stacked model input dataset as output from the ensemble\n",
    "def stacked_dataset(members, inputX):\n",
    "    stackX = None\n",
    "    for model in members:\n",
    "        yhat = model.predict(inputX, verbose=0)\n",
    "        if stackX is None:\n",
    "            stackX = yhat\n",
    "        else:\n",
    "            stackX = dstack((stackX, yhat))\n",
    "    # flatten predictions to [rows, members * probabilities]\n",
    "    stackX = stackX.reshape((stackX.shape[0], stackX.shape[1] * stackX.shape[2]))\n",
    "    return stackX\n",
    "\n",
    "# fit the model based on the outputs from the ensemble members\n",
    "def fit_stacked_model(members, inputX, inputy):\n",
    "    # create dataset\n",
    "    stackedX = stacked_dataset(members, inputX)\n",
    "    # fit model\n",
    "    model = LogisticRegression()\n",
    "    model.fit(stackedX, inputy)\n",
    "    return model\n",
    "\n",
    "# make prediction with the stacked model\n",
    "def stacked_prediction(members, model, inputX):\n",
    "    stackedX = stacked_dataset(members, inputX)\n",
    "    yhat = model.predict(stackedX)\n",
    "    return yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a4f5f4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 2) (1000, 2)\n",
      "loaded models/model_0.h5\n",
      "loaded models/model_1.h5\n",
      "loaded models/model_2.h5\n",
      "loaded models/model_3.h5\n",
      "loaded models/model_4.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-15 21:29:20.834213: I tensorflow/stream_executor/cuda/cuda_blas.cc:1774] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model acc: 0.803\n",
      "model acc: 0.803\n",
      "model acc: 0.807\n",
      "model acc: 0.816\n",
      "model acc: 0.807\n",
      "Stacked test acc: 0.834\n"
     ]
    }
   ],
   "source": [
    "## main state, loading sub-models into a list and evaluating each performance. the best is 81.6%\n",
    "## next, the logistic regression meta-learner is trained on the predicted probabilities from each sub-model on the\n",
    "## test set, then the entire stacking model is evaluated on the test set.\n",
    "# setup datasets\n",
    "trainX, trainy, testX, testy = prepare_data()\n",
    "print(trainX.shape, testX.shape)\n",
    "\n",
    "n_members = 5\n",
    "members = load_models(n_members) # load saved models\n",
    "\n",
    "# evaluate keras sub-models on test datasets\n",
    "for model in members:\n",
    "    testy_cat = to_categorical(testy)    \n",
    "    _, acc = model.evaluate(testX, testy_cat, verbose=0)\n",
    "    print('model acc: %.3f' % acc)\n",
    "\n",
    "# fit stacked models using ensemble\n",
    "model = fit_stacked_model(members, testX, testy)\n",
    "# evaluate model on test datasets\n",
    "yhat = stacked_prediction(members, model, testX)\n",
    "acc = accuracy_score(testy, yhat)\n",
    "print('Stacked test acc: %.3f' % acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eee7ac3",
   "metadata": {},
   "source": [
    "# Integrated stacking model\n",
    "When using neutral network as sub-models\n",
    "\n",
    "Specifically, the sub-networks can be embedded in a larger multi-headed neural network that then learns how to best combine the predictions from each input sub-model. It allows the stacking ensemble to be treated as a single large model.\n",
    "\n",
    "The benefit of this approach is that the outputs of the submodels are provided directly to the meta-learner. Further, it is also possible to update the weights of the submodels in conjunction with the meta-learner model, if this is desirable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4374f25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.layers import concatenate\n",
    "from numpy import argmax\n",
    "\n",
    "# stacked generalization with neutral network meta-learning model\n",
    "# re-define stacked model from multiple member input models\n",
    "def define_stacked_model(members):\n",
    "    # update all layers in all model to not be trainable\n",
    "    for i in range(len(members)):\n",
    "        model = members[i]\n",
    "        for layer in model.layers:\n",
    "            # not trainable\n",
    "            layer.trainable = False\n",
    "            # rename to avoid unique layer name issue\n",
    "            layer._name = 'ensemble_' + str(i+1) + layer.name\n",
    "    # define multi-headed input\n",
    "    ensemble_visible = [model.input for model in members]\n",
    "    # concatenate merge output from each model\n",
    "    ensemble_outputs = [model.output for model in members]\n",
    "    merge = concatenate(ensemble_outputs)\n",
    "    hidden = Dense(10, activation='relu')(merge) # set the input layer of model\n",
    "    output = Dense(3, activation='softmax')(hidden)\n",
    "    model = Model(inputs=ensemble_visible, outputs=output)\n",
    "    # plot graph of ensemble\n",
    "    plot_model(model, show_shapes=True, to_file='ensemble_stacked_model.png')\n",
    "    # compile\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# fit a stacked model\n",
    "def fit_stacked_model_int(model, inputX, inputy):\n",
    "    # prepare input data\n",
    "    X = [inputX for _ in range(len(model.input))]\n",
    "    # encode output data\n",
    "    y = to_categorical(inputy)\n",
    "    # fit model\n",
    "    model.fit(X, y, epochs=300, verbose=0)\n",
    "    \n",
    "# make prediction with stacked model\n",
    "def predict_stacked_model(model, inputX):\n",
    "    # prepare input data\n",
    "    X = [inputX for _ in range(len(model.input))]\n",
    "    # make prediction\n",
    "    return model.predict(X, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db2e7896",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 2) (1000, 2)\n",
      "loaded models/model_0.h5\n",
      "loaded models/model_1.h5\n",
      "loaded models/model_2.h5\n",
      "loaded models/model_3.h5\n",
      "loaded models/model_4.h5\n",
      "Stacked test acc: 0.832\n"
     ]
    }
   ],
   "source": [
    "## main stage,\n",
    "# generate data\n",
    "trainX, trainy, testX, testy = prepare_data()\n",
    "print(trainX.shape, testX.shape)\n",
    "# load sub-models\n",
    "n_models = 5\n",
    "members = load_models(n_models)\n",
    "# define ensemble model\n",
    "stacked_model = define_stacked_model(members)\n",
    "# fit stacked model on test set\n",
    "fit_stacked_model_int(stacked_model, testX, testy)\n",
    "# predict and evaluate model\n",
    "yhat = predict_stacked_model(stacked_model, testX)\n",
    "yhat = argmax(yhat, axis=1)\n",
    "\n",
    "acc = accuracy_score(testy, yhat)\n",
    "print('Stacked test acc: %.3f' % acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74a8538",
   "metadata": {},
   "source": [
    "Once the sub-models have been prepared, we can define the stacking ensemble model.\n",
    "\n",
    "The input layer for each of the sub-models will be used as a separate input head to this new model. This means that k copies of any input data will have to be provided to the model, where k is the number of input models, in this case, 5.\n",
    "\n",
    "The outputs of each of the models can then be merged. In this case, we will use a simple concatenation merge, where a single 15-element vector will be created from the three class-probabilities predicted by each of the 5 models.\n",
    "\n",
    "We will then define a hidden layer to interpret this “input” to the meta-learner and an output layer that will make its own probabilistic prediction. The define_stacked_model() function below implements this and will return a stacked generalization neural network model given a list of trained sub-models.\n",
    "\n",
    "# learning curve for diagnosing machine learning performance\n",
    "\n",
    "https://machinelearningmastery.com/learning-curves-for-diagnosing-machine-learning-model-performance/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa88cfc4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
