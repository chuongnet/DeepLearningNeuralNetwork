{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0faa2ace",
   "metadata": {},
   "source": [
    "# pre-Train word2vec Embedding with CNN model\n",
    "word2vec algorithm is an approach to learning a word embedding from a text corpus in a standalone way. The benefit of this method is reduce high-quality word embeddings, in terms of space and time complexity.\n",
    "\n",
    "word2vec algorithm processes document by sentences. Passing cleaned sentences from the training data with specify the size of the embedding vector space (vector_size=100), the number of words' correlation will take into account (window=5) maximizing, number of CPU cores (workers=4) or unsetting, and the minimun occurence count for word consider in the vocabulary (min_count=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec0f4827",
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "from os import listdir\n",
    "from gensim.models import Word2Vec\n",
    "# from nltk.corpus import stopwords\n",
    "from nltk import sent_tokenize\n",
    "\n",
    "\n",
    "def load_doc(fn):\n",
    "    file = open(fn, 'r')\n",
    "    doc = file.read()\n",
    "    file.close()\n",
    "    return doc\n",
    "\n",
    "\n",
    "def clean_doc(doc, vocab):\n",
    "    lines = list()\n",
    "    sentences = sent_tokenize(doc)\n",
    "    for sent in sentences:\n",
    "        tokens = sent.split()\n",
    "        table = str.maketrans('', '', punctuation)\n",
    "        tokens = [w.translate(table) for w in tokens]\n",
    "        tokens = [w for w in tokens if w in vocab]\n",
    "        lines.append(tokens)\n",
    "    return lines\n",
    "\n",
    "\n",
    "def process_docs(directory, vocab, is_train):\n",
    "    lines = list()\n",
    "    for fn in listdir(directory):\n",
    "        if is_train and fn.startswith('cv9'):\n",
    "            continue\n",
    "        if not is_train and not fn.startswith('cv9'):\n",
    "            continue\n",
    "        path = directory + '/' + fn\n",
    "        doc = load_doc(path)\n",
    "        tokens = clean_doc(doc, vocab)\n",
    "        lines += tokens\n",
    "    return lines\n",
    "\n",
    "vocab = load_doc('pickled_data/vocab.txt')\n",
    "vocab = set(vocab.split('\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76a8ee53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64190 [['dont', 'think', 'kevin', 'kline', 'drag', 'funny', 'wait'], ['til', 'see', 'smith', 'even', 'less', 'funny'], ['time', 'jim', 'west', 'smith', 'disguised', 'belly', 'dancer', 'bail', 'captured', 'comrade', 'artemus', 'gordon', 'kline', 'clutches', 'evil', 'dr'], ['loveless', 'branagh', 'unequivocally', 'bored', 'wild', 'wild', 'west', 'new', 'summer', 'blockbuster', 'men', 'black', 'director', 'barry', 'sonnenfeld'], ['old', 'west', 'really', 'breeding', 'ground', 'high', 'comedy', 'anyway']]\n"
     ]
    }
   ],
   "source": [
    "pos = process_docs('data/txt_sentoken/pos', vocab, True)\n",
    "neg = process_docs('data/txt_sentoken/neg', vocab, True)\n",
    "sentences = neg + pos\n",
    "print(len(sentences), sentences[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e104f400",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training model\n",
    "model = Word2Vec(sentences, vector_size=100, window=5, workers=3, min_count=1, epochs=50)\n",
    "words = model.wv.key_to_index\n",
    "print('vocab size= %d' % len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f44d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "## save model\n",
    "# model.save('pickled_data/word2vec_embedding.mod')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b83c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "## save wv in text\n",
    "# model.wv.save_word2vec_format('embedding_w2v.txt', binary=False, write_header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d1345bb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26896"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "# load and convert w2v vector spaces into embedding layer\n",
    "def load_embedding(fn):\n",
    "    file = open(fn, 'r')\n",
    "    lines = file.readlines()\n",
    "    file.close()\n",
    "    embedding = dict()\n",
    "    for line in lines:\n",
    "        tokens = line.split()\n",
    "        embedding[tokens[0]] = np.asarray(tokens[1:], dtype='float32')\n",
    "        \n",
    "    return embedding\n",
    "\n",
    "\n",
    "# create a weight mextrix for embedding\n",
    "def get_weight_matrix(embedding, vocab):\n",
    "    vocab_size = len(vocab) + 1\n",
    "    weight_matrix = np.zeros((vocab_size, 100))\n",
    "    for word, i in vocab.items():\n",
    "        # Tokenizer's integer mapping for get vocab\n",
    "        weight_matrix[i] = embedding.get(word)\n",
    "    return weight_matrix\n",
    "\n",
    "embedding = load_embedding('embedding_w2v.txt')\n",
    "display(len(embedding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3079fba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Dense, Flatten, Embedding, Conv1D, MaxPooling1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "895a76b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64190, 553)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(64190,)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([11,  8,  6,  5,  5, 15,  2,  6, 22, 22,  1, 21,  2,  6, 22,  9,  2,\n",
       "        6,  1, 11,  7,  3, 14, 18, 13,  6,  6, 17, 20,  3,  2,  5,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0], dtype=int32)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# max length of training docs\n",
    "max_length = max([len(s) for s in sentences])\n",
    "# convert tokens to lines for Tokenizer\n",
    "sentences = [' '.join(sent) for sent in sentences]\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "\n",
    "# encoding training docs\n",
    "encoded_docs = tokenizer.texts_to_sequences(sentences)\n",
    "\n",
    "# pad sequences\n",
    "Xtrain = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "ytrain = np.array([0 for _ in range(len(neg))] + [1 for _ in range(len(pos))])\n",
    "display(Xtrain.shape, ytrain.shape, Xtrain[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "29030352",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7342, 553)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(7342,)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0], dtype=int32)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# loading test sets\n",
    "pos = process_docs('data/txt_sentoken/pos', vocab, False)\n",
    "neg = process_docs('data/txt_sentoken/neg', vocab, False)\n",
    "sentences = neg + pos\n",
    "sentences = [' '.join(sent) for sent in sentences]\n",
    "\n",
    "# encoding test sets\n",
    "encoded_docs = tokenizer.texts_to_sequences(sentences)\n",
    "# pad sequences\n",
    "Xtest = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "ytest = np.array([0 for _ in range(len(neg))] + [1 for _ in range(len(pos))])\n",
    "display(Xtest.shape, ytest.shape, Xtest[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "848db6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "# convert the embedding to vector and transform with a Embedding() layer\n",
    "embedding_vectors = get_weight_matrix(embedding, tokenizer.word_index)\n",
    "embedding_layer = Embedding(vocab_size, 100, weights=[embedding_vectors], \n",
    "                            input_length=max_length, trainable=False)\n",
    "\n",
    "## trainable=False to ensure network does not try to adap the pre-learned vectors as a part of training network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0f028750",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 553, 100)          2700      \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 549, 128)          64128     \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 274, 128)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 35072)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 35073     \n",
      "=================================================================\n",
      "Total params: 101,901\n",
      "Trainable params: 99,201\n",
      "Non-trainable params: 2,700\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# define keras model\n",
    "model = Sequential()\n",
    "model.add(embedding_layer)\n",
    "# kernel_size = window in w2v for neighbors of word\n",
    "model.add(Conv1D(filters=128, kernel_size=5, activation='relu')) \n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ad1d5846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "2006/2006 - 152s - loss: nan - accuracy: 0.4966\n",
      "Epoch 2/5\n",
      "2006/2006 - 142s - loss: nan - accuracy: 0.4965\n",
      "Epoch 3/5\n",
      "2006/2006 - 142s - loss: nan - accuracy: 0.4965\n",
      "Epoch 4/5\n",
      "2006/2006 - 153s - loss: nan - accuracy: 0.4965\n",
      "Epoch 5/5\n",
      "2006/2006 - 154s - loss: nan - accuracy: 0.4965\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc5878617d0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# train model\n",
    "model.fit(Xtrain, ytrain, epochs=5, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2b2b254a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[nan, 0.4934622645378113]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(Xtest, ytest, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6225ad6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "model.save('pickled_data/embedding_cnn.h5') # indicate saved file under HDF5 optimizer\n",
    "\n",
    "## for load model\n",
    "# #import tensorflow as tf\n",
    "\n",
    "# #new_model = tf.keras.models.load_model('pickled_data/embedding_cnn.h5')\n",
    "# #new_model.summary()\n",
    "# #loss, acc = new_model.evaluate(Xtesting, ytesting, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa0c76b",
   "metadata": {},
   "source": [
    "### Summarizing Results\n",
    "Note: Your results may vary given the stochastic nature of the algorithm or evaluation procedure, or differences in numerical precision. Consider running the example a few times and compare the average outcome.\n",
    "\n",
    "In fact, performance was a lot worse. The results show that the training dataset was learned successfully, but evaluation on the test dataset was very poor, at just above 50% accuracy.\n",
    "\n",
    "The cause of the poor test performance may be because of the chosen word2vec configuration or the chosen neural network configuration.\n",
    "\n",
    "The weights in the embedding layer can be used as a starting point for the network, and adapted during the training of the network. We can do this by setting ‘trainable=True‘ (the default) in the creation of the embedding layer.\n",
    "\n",
    "Repeating the experiment with this change shows slightly better results, but still poor.\n",
    "\n",
    "It is possible to use pre-trained word vectors prepared on very large corpora of text data.\n",
    "\n",
    "For example, both Google and Stanford provide pre-trained word vectors that you can download, trained with the efficient word2vec and GloVe methods respectively.\n",
    "\n",
    "We can download pre-trained GloVe vectors from the Stanford webpage. Specifically, vectors trained on Wikipedia data (glove.6B.zip (http://nlp.stanford.edu/data/glove.6B.zip)) --> glove.6B.txt\n",
    "\n",
    "### Clearly understand further reading\n",
    "Implementing a CNN for Text Classification in TensorFlow\n",
    "http://www.wildml.com/2015/12/implementing-a-cnn-for-text-classification-in-tensorflow/\n",
    "\n",
    "RNNs in Tensorflow, a Practical Guide and Undocumented Features\n",
    "http://www.wildml.com/2016/08/rnns-in-tensorflow-a-practical-guide-and-undocumented-features/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06228b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a weight matrix for the Embedding layer from a loaded embedding\n",
    "def get_weight_matrix(embedding, vocab):\n",
    "    # total vocabulary size plus 0 for unknown words\n",
    "    vocab_size = len(vocab) + 1\n",
    "    # define weight matrix dimensions with all 0\n",
    "    weight_matrix = np.zeros((vocab_size, 100))\n",
    "    # step vocab, store vectors using the Tokenizer's integer mapping\n",
    "    for word, i in vocab.items():\n",
    "    vector = embedding.get(word)\n",
    "        if vector is not None:\n",
    "            weight_matrix[i] = vector\n",
    "    return weight_matrix\n",
    "\n",
    "# # load embedding from file\n",
    "# raw_embedding = load_embedding('glove.6B.100d.txt')\n",
    "# # get vectors in the right order\n",
    "# embedding_vectors = get_weight_matrix(raw_embedding, tokenizer.word_index)\n",
    "# # create the embedding layer\n",
    "# embedding_layer = Embedding(vocab_size, 100, weights=[embedding_vectors], \n",
    "#                             input_length=max_length, trainable=False)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e953bcf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5668f24a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
