{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "818ea2bf",
   "metadata": {},
   "source": [
    "# World Level Neural Language Model and Use to Generate Text\n",
    "Language model can predict the probability of the next word in the sequence, based on the words already observed in the sequence\n",
    "\n",
    "statistical language models, distributed representation where different words with similar meanings have similar representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cdf8370b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Project Gutenberg EBook of The Republic, by Plato\n",
      "\n",
      "This eBook is for the use of anyone anywhere at no cost and with\n",
      "almost no restrictions whatsoever.  You may copy it, give it away or\n",
      "re-use it u\n",
      "['the', 'project', 'gutenberg', 'ebook', 'of', 'the', 'republic', 'by', 'plato', 'this', 'ebook', 'is', 'for', 'the', 'use', 'of', 'anyone', 'anywhere', 'at', 'no', 'cost', 'and', 'with', 'almost', 'no', 'restrictions', 'whatsoever', 'you', 'may', 'copy', 'it', 'give', 'it', 'away', 'or', 'reuse', 'it', 'under', 'the', 'terms', 'of', 'the', 'project', 'gutenberg', 'license', 'included', 'with', 'this', 'ebook', 'or', 'online', 'at', 'wwwgutenbergorg', 'title', 'the', 'republic', 'author', 'plato', 'translator', 'b', 'jowett', 'posting', 'date', 'august', 'ebook', 'release', 'date', 'october', 'last', 'updated', 'june', 'language', 'english', 'start', 'of', 'this', 'project', 'gutenberg', 'ebook', 'the', 'republic', 'produced', 'by', 'sue', 'asscher', 'the', 'republic', 'by', 'plato', 'translated', 'by', 'benjamin', 'jowett', 'note', 'the', 'republic', 'by', 'plato', 'jowett', 'etext', 'introduction', 'and', 'analysis', 'the', 'republic', 'of', 'plato', 'is', 'the', 'longest', 'of', 'his', 'works', 'with', 'the', 'exception', 'of', 'the', 'laws', 'and', 'is', 'certainly', 'the', 'greatest', 'of', 'them', 'there', 'are', 'nearer', 'approaches', 'to', 'modern', 'metaphysics', 'in', 'the', 'philebus', 'and', 'in', 'the', 'sophist', 'the', 'politicus', 'or', 'statesman', 'is', 'more', 'ideal', 'the', 'form', 'and', 'institutions', 'of', 'the', 'state', 'are', 'more', 'clearly', 'drawn', 'out', 'in', 'the', 'laws', 'as', 'works', 'of', 'art', 'the', 'symposium', 'and', 'the', 'protagoras', 'are', 'of', 'higher', 'excellence', 'but', 'no', 'other', 'dialogue', 'of', 'plato', 'has', 'the', 'same', 'largeness', 'of', 'view', 'and', 'the', 'same', 'perfection', 'of', 'style', 'no', 'other', 'shows', 'an', 'equal', 'knowledge', 'of']\n",
      "Total Tokens: 216791\n",
      "Unique Tokens: 10454\n",
      "Total Sequences: 216740\n"
     ]
    }
   ],
   "source": [
    "from string import punctuation\n",
    "\n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n",
    " \n",
    "# turn a doc into clean tokens\n",
    "def clean_doc(doc):\n",
    "\t# replace '--' with a space ' '\n",
    "    doc = doc.replace('--', ' ')\n",
    "\t# split into tokens by white space\n",
    "    tokens = doc.split()\n",
    "\t# remove punctuation from each token\n",
    "    table = str.maketrans('', '', punctuation)\n",
    "    tokens = [w.translate(table) for w in tokens]\n",
    "\t# remove remaining tokens that are not alphabetic\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "\t# make lower case\n",
    "    tokens = [word.lower() for word in tokens]\n",
    "    return tokens\n",
    " \n",
    "# save tokens to file, one dialog per line\n",
    "def save_doc(lines, filename):\n",
    "    data = '\\n'.join(lines)\n",
    "    file = open(filename, 'w')\n",
    "    file.write(data)\n",
    "    file.close()\n",
    " \n",
    "# load document\n",
    "in_filename = 'data/republic_clean.txt'\n",
    "doc = load_doc(in_filename)\n",
    "print(doc[:200])\n",
    " \n",
    "# clean document\n",
    "tokens = clean_doc(doc)\n",
    "print(tokens[:200])\n",
    "print('Total Tokens: %d' % len(tokens))\n",
    "print('Unique Tokens: %d' % len(set(tokens)))\n",
    " \n",
    "# organize into sequences of tokens\n",
    "length = 50 + 1\n",
    "sequences = list()\n",
    "for i in range(length, len(tokens)):\n",
    "    # select sequence of tokens\n",
    "    seq = tokens[i-length:i]\n",
    "    # convert into a line\n",
    "    line = ' '.join(seq)\n",
    "    # store\n",
    "    sequences.append(line)\n",
    "print('Total Sequences: %d' % len(sequences))\n",
    " \n",
    "# save sequences to file\n",
    "out_filename = 'republic_sequences.txt'\n",
    "save_doc(sequences, out_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "218a6288",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from pickle import dump\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Embedding\n",
    "\n",
    "# load file\n",
    "doc = load_doc('republic_sequences.txt')\n",
    "lines = doc.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7485663",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "10455"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# integer encode sequences\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(lines)\n",
    "sequences = tokenizer.texts_to_sequences(lines) # convert texts to sequent numbers\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "display(type(sequences), vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d3a093f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1,\n",
       "  6444,\n",
       "  6443,\n",
       "  3810,\n",
       "  2,\n",
       "  1,\n",
       "  228,\n",
       "  23,\n",
       "  89,\n",
       "  31,\n",
       "  3810,\n",
       "  5,\n",
       "  26,\n",
       "  1,\n",
       "  174,\n",
       "  2,\n",
       "  6447,\n",
       "  3207,\n",
       "  34,\n",
       "  44,\n",
       "  4720,\n",
       "  3,\n",
       "  28,\n",
       "  572,\n",
       "  44,\n",
       "  6446,\n",
       "  10454,\n",
       "  22,\n",
       "  37,\n",
       "  2452,\n",
       "  73,\n",
       "  241,\n",
       "  73,\n",
       "  249,\n",
       "  13,\n",
       "  10453,\n",
       "  73,\n",
       "  165,\n",
       "  1,\n",
       "  511,\n",
       "  2,\n",
       "  1,\n",
       "  6444,\n",
       "  6443,\n",
       "  10452,\n",
       "  1565,\n",
       "  28,\n",
       "  31,\n",
       "  3810,\n",
       "  13,\n",
       "  6448],\n",
       " [6444,\n",
       "  6443,\n",
       "  3810,\n",
       "  2,\n",
       "  1,\n",
       "  228,\n",
       "  23,\n",
       "  89,\n",
       "  31,\n",
       "  3810,\n",
       "  5,\n",
       "  26,\n",
       "  1,\n",
       "  174,\n",
       "  2,\n",
       "  6447,\n",
       "  3207,\n",
       "  34,\n",
       "  44,\n",
       "  4720,\n",
       "  3,\n",
       "  28,\n",
       "  572,\n",
       "  44,\n",
       "  6446,\n",
       "  10454,\n",
       "  22,\n",
       "  37,\n",
       "  2452,\n",
       "  73,\n",
       "  241,\n",
       "  73,\n",
       "  249,\n",
       "  13,\n",
       "  10453,\n",
       "  73,\n",
       "  165,\n",
       "  1,\n",
       "  511,\n",
       "  2,\n",
       "  1,\n",
       "  6444,\n",
       "  6443,\n",
       "  10452,\n",
       "  1565,\n",
       "  28,\n",
       "  31,\n",
       "  3810,\n",
       "  13,\n",
       "  6448,\n",
       "  34],\n",
       " [6443,\n",
       "  3810,\n",
       "  2,\n",
       "  1,\n",
       "  228,\n",
       "  23,\n",
       "  89,\n",
       "  31,\n",
       "  3810,\n",
       "  5,\n",
       "  26,\n",
       "  1,\n",
       "  174,\n",
       "  2,\n",
       "  6447,\n",
       "  3207,\n",
       "  34,\n",
       "  44,\n",
       "  4720,\n",
       "  3,\n",
       "  28,\n",
       "  572,\n",
       "  44,\n",
       "  6446,\n",
       "  10454,\n",
       "  22,\n",
       "  37,\n",
       "  2452,\n",
       "  73,\n",
       "  241,\n",
       "  73,\n",
       "  249,\n",
       "  13,\n",
       "  10453,\n",
       "  73,\n",
       "  165,\n",
       "  1,\n",
       "  511,\n",
       "  2,\n",
       "  1,\n",
       "  6444,\n",
       "  6443,\n",
       "  10452,\n",
       "  1565,\n",
       "  28,\n",
       "  31,\n",
       "  3810,\n",
       "  13,\n",
       "  6448,\n",
       "  34,\n",
       "  6449],\n",
       " [3810,\n",
       "  2,\n",
       "  1,\n",
       "  228,\n",
       "  23,\n",
       "  89,\n",
       "  31,\n",
       "  3810,\n",
       "  5,\n",
       "  26,\n",
       "  1,\n",
       "  174,\n",
       "  2,\n",
       "  6447,\n",
       "  3207,\n",
       "  34,\n",
       "  44,\n",
       "  4720,\n",
       "  3,\n",
       "  28,\n",
       "  572,\n",
       "  44,\n",
       "  6446,\n",
       "  10454,\n",
       "  22,\n",
       "  37,\n",
       "  2452,\n",
       "  73,\n",
       "  241,\n",
       "  73,\n",
       "  249,\n",
       "  13,\n",
       "  10453,\n",
       "  73,\n",
       "  165,\n",
       "  1,\n",
       "  511,\n",
       "  2,\n",
       "  1,\n",
       "  6444,\n",
       "  6443,\n",
       "  10452,\n",
       "  1565,\n",
       "  28,\n",
       "  31,\n",
       "  3810,\n",
       "  13,\n",
       "  6448,\n",
       "  34,\n",
       "  6449,\n",
       "  1998],\n",
       " [2,\n",
       "  1,\n",
       "  228,\n",
       "  23,\n",
       "  89,\n",
       "  31,\n",
       "  3810,\n",
       "  5,\n",
       "  26,\n",
       "  1,\n",
       "  174,\n",
       "  2,\n",
       "  6447,\n",
       "  3207,\n",
       "  34,\n",
       "  44,\n",
       "  4720,\n",
       "  3,\n",
       "  28,\n",
       "  572,\n",
       "  44,\n",
       "  6446,\n",
       "  10454,\n",
       "  22,\n",
       "  37,\n",
       "  2452,\n",
       "  73,\n",
       "  241,\n",
       "  73,\n",
       "  249,\n",
       "  13,\n",
       "  10453,\n",
       "  73,\n",
       "  165,\n",
       "  1,\n",
       "  511,\n",
       "  2,\n",
       "  1,\n",
       "  6444,\n",
       "  6443,\n",
       "  10452,\n",
       "  1565,\n",
       "  28,\n",
       "  31,\n",
       "  3810,\n",
       "  13,\n",
       "  6448,\n",
       "  34,\n",
       "  6449,\n",
       "  1998,\n",
       "  1]]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(sequences[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ff50025",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "216740"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "216740"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sequences = array(sequences)\n",
    "# split into input and output\n",
    "X, y = sequences[:, :-1], sequences[:, -1]\n",
    "y = to_categorical(y, num_classes=vocab_size) # one-hot encode the output words for each input-output pairs\n",
    "seq_length = X.shape[1]\n",
    "display(len(X),len(y), seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4bd41f26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 50, 50)            522750    \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 50, 100)           60400     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10455)             1055955   \n",
      "=================================================================\n",
      "Total params: 1,729,605\n",
      "Trainable params: 1,729,605\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 50, input_length=seq_length)) # input is embedding\n",
    "model.add(LSTM(100, return_sequences=True)) # hidden 1\n",
    "model.add(LSTM(100))  # hidden 2\n",
    "model.add(Dense(100, activation='relu')) # hidden 3, sort out sequences\n",
    "model.add(Dense(vocab_size, activation='softmax')) # output is multiclass\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e7ec963",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1694/1694 [==============================] - 371s 213ms/step - loss: 6.1425 - accuracy: 0.0977\n",
      "Epoch 2/5\n",
      "1694/1694 [==============================] - 352s 208ms/step - loss: 5.6473 - accuracy: 0.1335\n",
      "Epoch 3/5\n",
      "1694/1694 [==============================] - 350s 206ms/step - loss: 5.4247 - accuracy: 0.1531\n",
      "Epoch 4/5\n",
      "1694/1694 [==============================] - 353s 208ms/step - loss: 5.2946 - accuracy: 0.1625\n",
      "Epoch 5/5\n",
      "1694/1694 [==============================] - 363s 214ms/step - loss: 5.1814 - accuracy: 0.1707\n"
     ]
    }
   ],
   "source": [
    "# the model learns a multiclass classification, the efficient Adam to mini-batch gradient descent, and uses\n",
    "# accuracy to evaluate of the model.\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X, y, batch_size=128, epochs=5)\n",
    "model.save('pickled_data/lang_model_generation.h5')\n",
    "# save tokenizer\n",
    "dump(tokenizer, open('pickled_data/tokenizer.pickled', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390b0a01",
   "metadata": {},
   "source": [
    "## Use the trained model to generate text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2c30baa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feeble person but sound in wind and limb and in perfect condition for the great gymnastic trial of the mind justice herself can find no fault with natures such as these and they will be the saviours of our state disciples of another sort would only make philosophy more ridiculous than\n",
      "\n",
      "the other and of the state and the same and the same and the same and the same and the same and the same and the same and the same and the same and the same and the same and the same and the same and the same and the\n"
     ]
    }
   ],
   "source": [
    "from random import randint\n",
    "from pickle import load\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "def generate_seq(model, tokenizer, seq_length, seed_text, n_words):\n",
    "    result = list()\n",
    "    in_text = seed_text\n",
    "    # generate fixed words \n",
    "    for _ in range(n_words):\n",
    "        # encode the text\n",
    "        encoded = tokenizer.texts_to_sequences([in_text])[0]\n",
    "        # truncate the sequence to a fixed length\n",
    "        encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
    "        # predict probabilities for each word\n",
    "        yhat = model.predict(encoded, verbose=0)\n",
    "        yhat = np.argmax(yhat, axis=-1)\n",
    "        out_word = ''\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == yhat:\n",
    "                out_word = word\n",
    "                break\n",
    "            \n",
    "        # append to input\n",
    "        in_text += ' ' + out_word\n",
    "        result.append(out_word)\n",
    "        \n",
    "    return ' '.join(result)\n",
    "\n",
    "# load cleaned text\n",
    "lines = load_doc('republic_sequences.txt').split('\\n')\n",
    "seq_length = len(lines[0].split()) - 1 # get length = 50\n",
    "\n",
    "#model = load_model('pickled_data/lang_model_generation.h5')\n",
    "#tokenizer = load(open('pickled_data/tokenizer.pickled', 'rb'))\n",
    "# select a seed text\n",
    "seed_text = lines[randint(0, len(lines))]\n",
    "print(seed_text + '\\n')\n",
    "\n",
    "# generate new text\n",
    "gen = generate_seq(model, tokenizer, seq_length, seed_text, 50)\n",
    "print(gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1f0bfd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
