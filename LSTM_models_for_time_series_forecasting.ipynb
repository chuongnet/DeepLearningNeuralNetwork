{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b00e3cd",
   "metadata": {},
   "source": [
    "# Develop LSTM Models for Time Series Forecasting\n",
    "A time series is a sequence of observations taken sequentially in time. Predictions are made for new data when the actual data may not be known until some future date.\n",
    "Understanding a dataset, called Time Series Analysis (depends on the purpose of prediction). \n",
    "\n",
    "In descriptive modeling, or time series analysis, a time series is modeled to determine its components in terms of seasonal patterns, trends, relation to external factors, and the like. … In contrast, time series forecasting uses the information in a time series (perhaps with additional information) to forecast future values of that series\n",
    "\n",
    "### Time Series Analysis\n",
    "Time series analysis involves developing models that best capture or describe an observed time series in order to understand the underlying causes. This field of study seeks the “why” behind a time series dataset.\n",
    "\n",
    "This often involves making assumptions about the form of the data and decomposing the time series into constitution components.\n",
    "\n",
    "### Time Series Forecasting\n",
    "Making predictions about the future is called extrapolation in the classical statistical handling of time series data.\n",
    "\n",
    "More modern fields focus on the topic and refer to it as time series forecasting.\n",
    "\n",
    "Forecasting involves taking models fit on historical data and using them to predict future observations.\n",
    "\n",
    "Descriptive models can borrow for the future (i.e. to smooth or remove noise), they only seek to best describe the data.\n",
    "\n",
    "An important distinction in forecasting is that the future is completely unavailable and must only be estimated from what has already happened.\n",
    "\n",
    "- The purpose of time series analysis is generally twofold: to understand or model the stochastic mechanisms that gives rise to an observed series and to predict or forecast the future values of a series based on the history of that series\n",
    "\n",
    "### Components of Time Series\n",
    "Time series analysis provides a body of techniques to better understand a dataset.\n",
    "\n",
    "Perhaps the most useful of these is the decomposition of a time series into 4 constituent parts:\n",
    "\n",
    "1. Level. The baseline value for the series if it were a straight line.\n",
    "2. Trend. The optional and often linear increasing or decreasing behavior of the series over time.\n",
    "3. Seasonality. The optional repeating patterns or cycles of behavior over time.\n",
    "4. Noise. The optional variability in the observations that cannot be explained by the model.\n",
    "\n",
    "All time series have a level, most have noise, and the trend and seasonality are optional.\n",
    "\n",
    "- The main features of many time series are trends and seasonal variations … another important feature of most time series is that observations close together in time tend to be correlated (serially dependent)\n",
    "\n",
    "### Concerns of Forecasting\n",
    "When forecasting, it is important to understand your goal.\n",
    "\n",
    "Use the Socratic method and ask lots of questions to help zoom in on the specifics of your predictive modeling problem. For example:\n",
    "\n",
    "1. How much data do you have available and are you able to gather it all together? More data is often more helpful, offering greater opportunity for exploratory data analysis, model testing and tuning, and model fidelity.\n",
    "2. What is the time horizon of predictions that is required? Short, medium or long term? Shorter time horizons are often easier to predict with higher confidence.\n",
    "3. Can forecasts be updated frequently over time or must they be made once and remain static? Updating forecasts as new information becomes available often results in more accurate predictions.\n",
    "4. At what temporal frequency are forecasts required? Often forecasts can be made at a lower or higher frequencies, allowing you to harness down-sampling, and up-sampling of data, which in turn can offer benefits while modeling.\n",
    "\n",
    "Time series data often requires cleaning, scaling, and even transformation.\n",
    "\n",
    "For example:\n",
    "\n",
    "- Frequency. Perhaps data is provided at a frequency that is too high to model or is unevenly spaced through time requiring resampling for use in some models.\n",
    "- Outliers. Perhaps there are corrupt or extreme outlier values that need to be identified and handled.\n",
    "- Missing. Perhaps there are gaps or missing data that need to be interpolated or imputed.\n",
    "\n",
    "Often time series problems are real-time, continually providing new opportunities for prediction. This adds an honesty to time series forecasting that quickly flushes out bad assumptions, errors in modeling and all the other ways that we may be able to fool ourselves.\n",
    "\n",
    "### Examples of Time Series Forecasting\n",
    "There is almost an endless supply of time series forecasting problems.\n",
    "\n",
    "Below are 10 examples from a range of industries to make the notions of time series analysis and forecasting more concrete.\n",
    "\n",
    "Forecasting the corn yield in tons by state each year.\n",
    "\n",
    "Forecasting whether an EEG trace in seconds indicates a patient is having a seizure or not.\n",
    "\n",
    "Forecasting the closing price of a stock each day.\n",
    "\n",
    "Forecasting the birth rate at all hospitals in a city each year.\n",
    "\n",
    "Forecasting product sales in units sold each day for a store.\n",
    "\n",
    "Forecasting the number of passengers through a train station each day.\n",
    "\n",
    "Forecasting unemployment for a state each quarter.\n",
    "\n",
    "Forecasting utilization demand on a server each hour.\n",
    "\n",
    "Forecasting the size of the rabbit population in a state each breeding season.\n",
    "\n",
    "Forecasting the average price of gasoline in a city each day.\n",
    "\n",
    "I expect that you will be able to relate one or more of these examples to your own time series forecasting problems that you would like to address."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc41bc1d",
   "metadata": {},
   "source": [
    "# LSTM models for Time Series Forecasting\n",
    "### How to develop a suite of LSTM models for a range of standard time series forecasting problems.\n",
    "- How to develop LSTM models for univariate time series forecasting.\n",
    "- How to develop LSTM models for multivariate time series forecasting.\n",
    "- How to develop LSTM models for multi-step time series forecasting.\n",
    "\n",
    "There are four parts of this tutorial:\n",
    "\n",
    "1. Univariate LSTM Models\n",
    "\n",
    "    1.1. Data Preparation\n",
    "\n",
    "    1.2. Vanilla LSTM\n",
    "    \n",
    "    1.3. Stacked LSTM\n",
    "    \n",
    "    1.4. Bidirectional LSTM\n",
    "    \n",
    "    1.5. CNN LSTM\n",
    "    \n",
    "    1.6. ConvLSTM\n",
    "\n",
    "2. Multivariate LSTM Models\n",
    "    \n",
    "    2.1. Multiple Input Series.\n",
    "    \n",
    "    2.2.Multiple Parallel Series.\n",
    "\n",
    "3. Multi-Step LSTM Models\n",
    "\n",
    "    3.1. Data Preparation\n",
    "    \n",
    "    3.2. Vector Output Model\n",
    "    \n",
    "    3.3. Encoder-Decoder Model\n",
    "\n",
    "4. Multivariate Multi-Step LSTM Models\n",
    "\n",
    "    4.1. Multiple Input Multi-Step Output.\n",
    "    \n",
    "    4.2. Multiple Parallel Input and Multi-Step Output.\n",
    "    \n",
    "# 1. Univariate LSTM models, the demonstration of the LSTM model for univariate time series forecasting\n",
    "The LSTM model will learn a function that maps a sequence of past observations as input to an output observation. As such, the sequence of observations must be transformed into multiple examples from which the LSTM can learn.\n",
    "\n",
    "### 1.1 Data preparation\n",
    "A given univariate sequence:\n",
    "\n",
    "    [10, 20, 30, 40, 50, 60, 70, 80, 90]\n",
    "    \n",
    "We can divide the sequence into multiple input/output patterns called samples, where three time steps are used as input and one time step is used as output for the one-step prediction that is being learned.\n",
    "\n",
    "    X,\t\t\t\ty\n",
    "    10, 20, 30\t\t40\n",
    "    20, 30, 40\t\t50\n",
    "    30, 40, 50\t\t60\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10f0fe07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10 20 30] 40\n",
      "[20 30 40] 50\n",
      "[30 40 50] 60\n",
      "[40 50 60] 70\n",
      "[50 60 70] 80\n",
      "[60 70 80] 90\n"
     ]
    }
   ],
   "source": [
    "# univariate data preparation\n",
    "from numpy import array\n",
    "\n",
    "# split a univariate sequence into samples\n",
    "def split_sequence(sequence, n_steps):\n",
    "\tX, y = list(), list()\n",
    "\tfor i in range(len(sequence)):\n",
    "\t\t# find the end of this pattern\n",
    "\t\tend_ix = i + n_steps\n",
    "\t\t# check if we are beyond the sequence\n",
    "\t\tif end_ix > len(sequence)-1:\n",
    "\t\t\tbreak\n",
    "\t\t# gather input and output parts of the pattern\n",
    "\t\tseq_x, seq_y = sequence[i:end_ix], sequence[end_ix]\n",
    "\t\tX.append(seq_x)\n",
    "\t\ty.append(seq_y)\n",
    "\treturn array(X), array(y)\n",
    "\n",
    "# define input sequence\n",
    "raw_seq = [10, 20, 30, 40, 50, 60, 70, 80, 90]\n",
    "# choose a number of time steps\n",
    "n_steps = 3\n",
    "# split into samples\n",
    "X, y = split_sequence(raw_seq, n_steps)\n",
    "# summarize the data\n",
    "for i in range(len(X)):\n",
    "\tprint(X[i], y[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1879b9c",
   "metadata": {},
   "source": [
    "### 1.2 Vanilla LSTM\n",
    "A Vanilla LSTM is an LSTM model that has a single hidden layer of LSTM units, and an output layer used to make a prediction.\n",
    "\n",
    "We can define a Vanilla LSTM for univariate time series forecasting as follows.\n",
    "\n",
    "Key in the definition is the shape of the input; that is what the model expects as input for each sample in terms of the number of time steps and the number of features.\n",
    "\n",
    "We are working with a univariate series, so the number of features is one, for one variable.\n",
    "\n",
    "The number of time steps as input is the number we chose when preparing our dataset as an argument to the split_sequence() function.\n",
    "\n",
    "The shape of the input for each sample is specified in the input_shape argument on the definition of first hidden layer.\n",
    "\n",
    "We almost always have multiple samples, therefore, the model will expect the input component of training data to have the dimensions or shape:\n",
    "\n",
    "    [samples, timesteps, features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35ae3cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7905bc45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10]\n",
      " [20]\n",
      " [30]]\n",
      "[[20]\n",
      " [30]\n",
      " [40]]\n",
      "[[30]\n",
      " [40]\n",
      " [50]]\n",
      "[[40]\n",
      " [50]\n",
      " [60]]\n",
      "[[50]\n",
      " [60]\n",
      " [70]]\n",
      "[[60]\n",
      " [70]\n",
      " [80]]\n"
     ]
    }
   ],
   "source": [
    "# reshape from [samples, timesteps] into [samples, timesteps, features]\n",
    "n_features = 1\n",
    "X = X.reshape((X.shape[0], X.shape[1], n_features))\n",
    "for val in X:\n",
    "    print(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98609048",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-21 16:09:49.653752: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error\n",
      "2022-02-21 16:09:49.653775: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: G15\n",
      "2022-02-21 16:09:49.653779: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: G15\n",
      "2022-02-21 16:09:49.653830: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 510.47.3\n",
      "2022-02-21 16:09:49.653842: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 510.47.3\n",
      "2022-02-21 16:09:49.653845: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 510.47.3\n",
      "2022-02-21 16:09:49.654057: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[70]\n",
      "  [80]\n",
      "  [90]]]\n",
      "[[102.874565]]\n"
     ]
    }
   ],
   "source": [
    "## define vanilla model\n",
    "# create Sequential model\n",
    "model = Sequential()\n",
    "# define 50 LSTM units in hidden layer, output layer predicts a single numerical value\n",
    "model.add(LSTM(50, activation='relu', input_shape=(n_steps, n_features)))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "# fit model\n",
    "model.fit(X, y, epochs=200, verbose=0)\n",
    "\n",
    "# demonstrate prediction\n",
    "x_input = array([70, 80, 90]) # test set\n",
    "x_input = x_input.reshape((1, n_steps, n_features))\n",
    "print(x_input)\n",
    "\n",
    "yhat = model.predict(x_input, verbose=0)\n",
    "print(yhat)\n",
    "\n",
    "## result: model will predict a next value in the sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0063252",
   "metadata": {},
   "source": [
    "- Expected value is 100\n",
    "\n",
    "### 1.2. Stacked LSTM\n",
    "Multiple hidden LSTM layers can be stacked one on top of another in what is referred to as a Stacked LSTM model.\n",
    "\n",
    "An LSTM layer requires a three-dimensional input and LSTMs by default will produce a two-dimensional output as an interpretation from the end of the sequence.\n",
    "\n",
    "We can address this by having the LSTM output a value for each time step in the input data by setting the return_sequences=True argument on the layer. This allows us to have 3D output from hidden LSTM layer as input to the next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "653ac599",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[103.7662]]\n"
     ]
    }
   ],
   "source": [
    "del model # delete model parameter out of RAM\n",
    "# redefine model\n",
    "model = Sequential()\n",
    "model.add(LSTM(50, activation='relu', return_sequences=True, input_shape=(n_steps, n_features)))\n",
    "model.add(LSTM(50, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "# compile\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "# fit \n",
    "model.fit(X, y, epochs=200, verbose=0)\n",
    "# demonstrate prediction\n",
    "# re-using x_input\n",
    "yhat = model.predict(x_input, verbose=0)\n",
    "print(yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9327e16",
   "metadata": {},
   "source": [
    "### 1.3. Bidirectional LSTM\n",
    "On some sequence prediction problems, it can be beneficial to allow the LSTM model to learn the input sequence both forward and backwards and concatenate both interpretations.\n",
    "\n",
    "This is called a Bidirectional LSTM.\n",
    "\n",
    "We can implement a Bidirectional LSTM for univariate time series forecasting by wrapping the first hidden layer in a wrapper layer called Bidirectional.\n",
    "\n",
    "An example of defining a Bidirectional LSTM to read input both forward and backward is as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75924326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[101.63932]]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Bidirectional\n",
    "\n",
    "\n",
    "del model\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Bidirectional(LSTM(50, activation='relu'), input_shape=(n_steps, n_features)))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "model.fit(X, y, epochs=200, verbose=0)\n",
    "# re-using x_input\n",
    "yhat = model.predict(x_input, verbose=0)\n",
    "print(yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dcca001",
   "metadata": {},
   "source": [
    "### 1.4. CNN LSTM\n",
    "A convolutional neural network, or CNN for short, is a type of neural network developed for working with two-dimensional image data.\n",
    "\n",
    "The CNN can be very effective at automatically extracting and learning features from one-dimensional sequence data such as univariate time series data.\n",
    "\n",
    "A CNN model can be used in a hybrid model with an LSTM backend where the CNN is used to interpret subsequences of input that together are provided as a sequence to an LSTM model to interpret. This hybrid model is called a CNN-LSTM.\n",
    "\n",
    "The first step is to split the input sequences into subsequences that can be processed by the CNN model. For example, we can first split our univariate time series data into input/output samples with four steps as input and one as output. Each sample can then be split into two sub-samples, each with two time steps. The CNN can interpret each subsequence of two time steps and provide a time series of interpretations of the subsequences to the LSTM model to process as input.\n",
    "\n",
    "We can parameterize this and define the number of subsequences as n_seq and the number of time steps per subsequence as n_steps. The input data can then be reshaped to have the required structure:\n",
    "\n",
    "    [samples, subsequences, timesteps, features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7840710a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 2, 2, 1)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[[10],\n",
       "        [20]],\n",
       "\n",
       "       [[30],\n",
       "        [40]]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# re-define input/output by choose a number of time steps\n",
    "n_steps = 4\n",
    "X, y = split_sequence(raw_seq, n_steps)\n",
    "# reshape from [samples, timestep] into [samples, subsequences, timestep, features]\n",
    "n_features = 1\n",
    "n_seq = 2\n",
    "n_steps = 2\n",
    "X = X.reshape((X.shape[0], n_seq, n_steps, n_features))\n",
    "display(X.shape, X[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149ee2a8",
   "metadata": {},
   "source": [
    "- Reuse the same CNN model when reading in each sub-sequence of data separately, by wrapping the entire CNN model in the TimeDistributed wrapper that will apply the entire model once per input.\n",
    "- The CNN first has convolutional layer for reading across the subsequence that requires a number of filters and a kernel size to be specified. The number of filters is the number of reads or interpretations of the input sequence. The kernel size is the number of time steps included of each ‘read’ operation of the input sequence.\n",
    "\n",
    "- The convolution layer is followed by a max pooling layer that distills the filter maps down to 1/2 of their size that includes the most salient features. These structures are then flattened down to a single one-dimensional vector to be used as a single input time step to the LSTM layer.\n",
    "- Next, we can define the LSTM part of the model that interprets the CNN model’s read of the input sequence and makes a prediction.\n",
    "\n",
    "#### CNN-LSTM model for univariate time series forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c014ba4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[101.72918]]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Flatten, TimeDistributed, Conv1D, MaxPooling1D\n",
    "\n",
    "del model\n",
    "# define the mixed CNN-LSTM model\n",
    "model = Sequential()\n",
    "# specific the CNN model has Conv1D layer\n",
    "model.add(TimeDistributed(Conv1D(filters=64, kernel_size=1, activation='relu'), input_shape=(None, n_steps, n_features)))\n",
    "# max pooling layer distills the filter maps down to 1/2 of size\n",
    "model.add(TimeDistributed(MaxPooling1D(pool_size=2)))\n",
    "# then, flatten the model to a single one dimensional vector to be used as a single input time step to the LSTM\n",
    "model.add(TimeDistributed(Flatten()))\n",
    "# next, model defines the LSTM that interprets the CNN model's read of the input sequence and make prediction\n",
    "model.add(LSTM(50, activation='relu'))\n",
    "model.add(Dense(1)) # output\n",
    "# compile model\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "# fit training data to model\n",
    "model.fit(X, y, epochs=500, verbose=0)\n",
    "\n",
    "# prediction on test set\n",
    "testX = array([60, 70, 80, 90])\n",
    "testX = testX.reshape((1, n_seq, n_steps, n_features))\n",
    "yhat= model.predict(testX, verbose=0)\n",
    "print(yhat)\n",
    "## expected value 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2714429",
   "metadata": {},
   "source": [
    "### ConvLSTM\n",
    "A type of LSTM related to the CNN-LSTM is the ConvLSTM, where the convolutional reading of input is built directly into each LSTM unit.\n",
    "\n",
    "The ConvLSTM was developed for reading two-dimensional spatial-temporal data, but can be adapted for use with univariate time series forecasting.\n",
    "\n",
    "The layer expects input as a sequence of two-dimensional images, therefore the shape of input data must be:\n",
    "\n",
    "    [samples, timesteps, rows, columns, features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "42a7af0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 2, 1, 2, 1)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# choose a number of time steps\n",
    "n_steps = 4\n",
    "# split into samples\n",
    "X, y = split_sequence(raw_seq, n_steps)\n",
    "# reshape from [samples, timesteps] into [samples, timesteps, rows, columns, features]\n",
    "n_features = 1\n",
    "n_seq = 2\n",
    "n_steps = 2\n",
    "X = X.reshape((X.shape[0], n_seq, 1, n_steps, n_features))\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b8bf16",
   "metadata": {},
   "source": [
    "We can define the ConvLSTM as a single layer in terms of the number of filters and a two-dimensional kernel size in terms of (rows, columns). As we are working with a one-dimensional series, the number of rows is always fixed to 1 in the kernel.\n",
    "\n",
    "The output of the model must then be flattened before it can be interpreted and a prediction made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d5c2f4ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f7924498af0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[[103.65026]]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import ConvLSTM2D\n",
    "\n",
    "del model\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(ConvLSTM2D(filters=64, kernel_size=(1,2), activation='relu', input_shape=(n_seq, 1, n_steps, n_features)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "# fit model\n",
    "model.fit(X, y, epochs=500, verbose=0)\n",
    "# demonstrate prediction\n",
    "testX = array([60, 70, 80, 90])\n",
    "testX = testX.reshape((1, n_seq, 1, n_steps, n_features))\n",
    "yhat = model.predict(testX, verbose=0)\n",
    "print(yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126769b1",
   "metadata": {},
   "source": [
    "# 2. Multivariate LSTM Models\n",
    "Multivariate time series data means data where there is more than one observation for each time step.\n",
    "\n",
    "There are two main models for the multivariate time series data:\n",
    "1. Multiple Input Series\n",
    "2. Multiple Parallel Series\n",
    "\n",
    "## 2.1 Multiple Input Series\n",
    "A problem may have two or more parallel input time series and an output time series that is dependent on the input time series.\n",
    "\n",
    "The input time series are parallel because each series has an observation at the same time steps.\n",
    "\n",
    "- We can reshape these three arrays of data as a single dataset where each row is a time step, and each column is a separate time series. This is a standard way of storing parallel time series in a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "228afd64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 10,  15,  25],\n",
       "       [ 20,  25,  45],\n",
       "       [ 30,  35,  65],\n",
       "       [ 40,  45,  85],\n",
       "       [ 50,  55, 105],\n",
       "       [ 60,  65, 125],\n",
       "       [ 70,  75, 145],\n",
       "       [ 80,  85, 165],\n",
       "       [ 90,  95, 185]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# define and reshape input sequences and output sequence.\n",
    "def define_seq(seq1, seq2):\n",
    "    seq1 = np.array(seq1)\n",
    "    seq2 = np.array(seq2)\n",
    "    output = np.array([seq1[i] + seq2[i] for i in range(len(seq1))])\n",
    "    # reshape to [rows, columns]\n",
    "    seq1 = seq1.reshape((len(seq1), 1)) # multi row, 1 column\n",
    "    seq2 = seq2.reshape((len(seq2), 1))\n",
    "    output = output.reshape((len(output), 1))\n",
    "    # compress 3 seqs into the horizontal structure seq and return\n",
    "    return np.hstack((seq1, seq2, output))\n",
    "\n",
    "# define input sequence\n",
    "in_seq1 = [10, 20, 30, 40, 50, 60, 70, 80, 90]\n",
    "in_seq2 = [15, 25, 35, 45, 55, 65, 75, 85, 95]\n",
    "\n",
    "dataset = define_seq(in_seq1, in_seq2)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c09850",
   "metadata": {},
   "source": [
    "As with the univariate time series, we must structure these data into samples with input and output elements.\n",
    "\n",
    "An LSTM model needs sufficient context to learn a mapping from an input sequence to an output value. LSTMs can support parallel input time series as separate variables or features. Therefore, we need to split the data into samples maintaining the order of observations across the two input sequences.\n",
    "\n",
    "if we chose three input time steps, so:\n",
    "    \n",
    "    10, 15\n",
    "    20, 25\n",
    "    30, 35\n",
    "    \n",
    "then the output series is 65\n",
    "\n",
    "We can see that, in transforming the time series into input/output samples to train the model, that we will have to discard some values from the output time series where we do not have values in the input time series at prior time steps. In turn, the choice of the size of the number of input time steps will have an important effect on how much of the training data is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "420d55e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7, 3, 2) (7,)\n",
      "[[10 15]\n",
      " [20 25]\n",
      " [30 35]] 65\n"
     ]
    }
   ],
   "source": [
    "# split a multivariate sequence into samples\n",
    "def split_sequences(sequences, n_steps):\n",
    "    X, y = list(), list()\n",
    "    for i in range(len(sequences)):\n",
    "        end_ix = i + n_steps\n",
    "        if end_ix > len(sequences):\n",
    "            break\n",
    "        seqX = sequences[i:end_ix, :-1]\n",
    "        seqy = sequences[end_ix - 1, -1]\n",
    "        X.append(seqX)\n",
    "        y.append(seqy)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "n_steps = 3\n",
    "X, y = split_sequences(dataset, n_steps)\n",
    "print(X.shape, y.shape)\n",
    "print(X[0], y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "27bc1db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## We will use a Vanilla LSTM where the number of time steps and parallel series (features) are specified \n",
    "## for the input layer via the input_shape argument. We can use any LSTM models such as Vanilla, \n",
    "## Stacked, Bidirection, CNN, ConvLSTM.\n",
    "\n",
    "# define model\n",
    "def vanilla_model(X, y, n_steps):\n",
    "    n_features = X.shape[2]\n",
    "    # define model\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(50, activation='relu', input_shape=(n_steps, n_features)))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    model.fit(X, y, epochs=500, verbose=0)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4b34abfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f791450de50> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[[205.62956]]\n"
     ]
    }
   ],
   "source": [
    "# predict test set\n",
    "n_features = X.shape[2]\n",
    "input_x = np.array([[80, 85], [90, 95], [100, 105]])\n",
    "input_x = input_x.reshape((1, n_steps, n_features))\n",
    "\n",
    "model = vanilla_model(X, y, n_steps)\n",
    "yhat = model.predict(input_x, verbose=0)\n",
    "print(yhat)\n",
    "\n",
    "## result is quietly well, which expected value is 205"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5e16fd",
   "metadata": {},
   "source": [
    "## 2.2 Multiple Parallel Series\n",
    "An alternate time series problem is the case where there are multiple parallel time series and a value must be predicted for each.\n",
    "\n",
    "We may want to predict the value for each of the three time series for the next time step.\n",
    "\n",
    "This might be referred to as multivariate forecasting.\n",
    "\n",
    "    10, 15, 25\n",
    "    20, 25, 45\n",
    "    30, 35, 65\n",
    "    \n",
    "Output:\n",
    "    \n",
    "    40, 45, 85"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8e181801",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 3, 3) (6, 3)\n"
     ]
    }
   ],
   "source": [
    "# re-define the split_sequences\n",
    "def split_parallel_sequences(sequences, n_steps):\n",
    "    X, y = list(), list()\n",
    "    for i in range(len(sequences)):\n",
    "        end_ix = i + n_steps\n",
    "        if end_ix > len(sequences) - 1:\n",
    "            break\n",
    "        # gather input and output parts of pattern\n",
    "        seq_x = sequences[i:end_ix, :]\n",
    "        seq_y = sequences[end_ix, :]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# process dataset\n",
    "n_steps = 3\n",
    "X, y = split_parallel_sequences(dataset, n_steps)\n",
    "n_features = X.shape[2]\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "09ad09af",
   "metadata": {},
   "outputs": [],
   "source": [
    "## We will use a Stacked LSTM where the number of time steps and parallel series (features) are specified for \n",
    "## the input layer via the input_shape argument. The number of parallel series is also used in the specification \n",
    "## of the number of values to predict by the model in the output layer. (can apply any models)\n",
    "\n",
    "# using the Stacked LSTM model\n",
    "def stacked_model(X, y, n_steps, n_features):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(100, activation='relu', return_sequences=True, input_shape=(n_steps, n_features)))\n",
    "    model.add(LSTM(100, activation='relu'))\n",
    "    model.add(Dense(n_features))\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    model.fit(X, y, epochs=500, verbose=0)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a6a9b595",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[100.60504 106.18641 206.44127]]\n"
     ]
    }
   ],
   "source": [
    "# predict on testset\n",
    "input_x = np.array([[70,75,145], [80,85,165], [90,95,185]])\n",
    "input_x = input_x.reshape((1, n_steps, n_features))\n",
    "model = stacked_model(X, y, n_steps, n_features)\n",
    "yhat = model.predict(input_x, verbose=0)\n",
    "print(yhat)\n",
    "\n",
    "# expected value 100, 105, 205"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f18730d",
   "metadata": {},
   "source": [
    "# 3. Multi-Step LSTM models\n",
    "A time series forecasting problem that requires a prediction of multiple time steps into the future can be referred to as multi-step time series forecasting.\n",
    "\n",
    "Specifically, these are problems where the forecast horizon or interval is more than one time step.\n",
    "\n",
    "There are two main types of LSTM models that can be used for multi-step forecasting\n",
    "\n",
    "1. Vector Output Model\n",
    "2. Encoder-Decoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "80f0a938",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10 20 30] [40 50]\n"
     ]
    }
   ],
   "source": [
    "## prepare data for input seq [10, 20, 30] and output seq [40, 50]\n",
    "def split_multi_steps(sequences, n_in, n_out):\n",
    "    X, y = list(), list()\n",
    "    for i in range(len(sequences)):\n",
    "        end_ix = i + n_in\n",
    "        out_end_ix = end_ix + n_out\n",
    "        if out_end_ix > len(sequences):\n",
    "            break\n",
    "        seq_x, seq_y = sequences[i:end_ix], sequences[end_ix:out_end_ix]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# split into samples\n",
    "n_steps_in, n_steps_out = 3, 2\n",
    "X, y = split_multi_steps(raw_seq, n_steps_in, n_steps_out) # use raw_seq \n",
    "print(X[0], y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2083f54a",
   "metadata": {},
   "source": [
    "## 3.1. Vector Output Model\n",
    "Like other types of neural network models, the LSTM can output a vector directly that can be interpreted as a multi-step forecast.\n",
    "\n",
    "This approach was seen in the previous section were one time step of each output time series was forecasted as a vector.\n",
    "\n",
    "As with the LSTMs for univariate data in a prior section, the prepared samples must first be reshaped. The LSTM expects data to have a three-dimensional structure of [samples, timesteps, features], and in this case, we only have one feature so the reshape is straightforward.\n",
    "\n",
    "With the number of input and output steps specified in the n_steps_in and n_steps_out variables, we can define a multi-step time-series forecasting model.\n",
    "\n",
    "Any of the presented LSTM model types could be used, such as Vanilla, Stacked, Bidirectional, CNN-LSTM, or ConvLSTM. Below defines a Stacked LSTM for multi-step forecasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a171f208",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape from [samples, timesteps] into [samples, timesteps, features]\n",
    "n_features = 1\n",
    "X = X.reshape((X.shape[0], X.shape[1], n_features))\n",
    "\n",
    "# define model, Stacked model type.\n",
    "def vector_output_model(X, y, n_steps_in, n_steps_out, n_features):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(100, activation='relu', return_sequences=True, input_shape=(n_steps_in, n_features)))\n",
    "    model.add(LSTM(100, activation='relu'))\n",
    "    model.add(Dense(n_steps_out))\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    model.fit(X, y, epochs=500, verbose=0)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7f3cfc4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[103.028175 114.33643 ]]\n"
     ]
    }
   ],
   "source": [
    "input_x = np.array([70, 80, 90])\n",
    "input_x = input_x.reshape((1, n_steps_in, n_features))\n",
    "# result test set\n",
    "model = vector_output_model(X, y, n_steps_in, n_steps_out, n_features)\n",
    "yhat = model.predict(input_x, verbose=0)\n",
    "print(yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46851ef",
   "metadata": {},
   "source": [
    "## 3.2 Encoder-Decoder Model\n",
    "A model specifically developed for forecasting variable length output sequences is called the Encoder-Decoder LSTM.\n",
    "\n",
    "The model was designed for prediction problems where there are both input and output sequences, so-called sequence-to-sequence, or seq2seq problems, such as translating text from one language to another.\n",
    "\n",
    "This model can be used for multi-step time series forecasting.\n",
    "\n",
    "As its name suggests, the model is comprised of two sub-models: the encoder and the decoder.\n",
    "\n",
    "The encoder is a model responsible for reading and interpreting the input sequence. The output of the encoder is a fixed length vector that represents the model’s interpretation of the sequence. The encoder is traditionally a Vanilla LSTM model, although other encoder models can be used such as Stacked, Bidirectional, and CNN models.\n",
    "\n",
    "The decoder uses the output of the encoder as an input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1d495333",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 3, 1) (5, 2)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import RepeatVector, TimeDistributed\n",
    "\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "54d66943",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 2, 1) [[40]\n",
      " [50]]\n"
     ]
    }
   ],
   "source": [
    "# univariate multi-step encoder-decoder lstm example\n",
    "# required the y reshape\n",
    "y = y.reshape((y.shape[0], y.shape[1], n_features))\n",
    "print(y.shape, y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "db9ad540",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define encoder-decoder model\n",
    "def encoder_decoder_model(X, y, n_steps_in, n_steps_out, n_features):\n",
    "    model = Sequential()\n",
    "    # encoder, reading and interpreting the input seq, the output of encoder is a fixed length vector\n",
    "    model.add(LSTM(100, activation='relu', input_shape=(n_steps_in, n_features)))\n",
    "    # repeating fixed length output of encoder with required time step in the output seq\n",
    "    model.add(RepeatVector(n_steps_out))\n",
    "    # LSTM decoder model, output value on each value in output time step\n",
    "    model.add(LSTM(100, activation='relu', return_sequences=True))\n",
    "    # use the same output layer for prediction in the output seq \n",
    "    model.add(TimeDistributed(Dense(1)))\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    model.fit(X, y, epochs=500, verbose=0)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3dcf1710",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[102.2284 ]\n",
      "  [112.68889]]]\n"
     ]
    }
   ],
   "source": [
    "# demonstrate prediction\n",
    "x_input = np.array([70, 80, 90])\n",
    "x_input = x_input.reshape((1, n_steps_in, n_features))\n",
    "model = encoder_decoder_model(X, y, n_steps_in, n_steps_out, n_features)\n",
    "yhat = model.predict(x_input, verbose=0)\n",
    "print(yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442ddc7d",
   "metadata": {},
   "source": [
    "# 4. Multivariate Multi-step LSTM models\n",
    "It is possible to mix and match the different types of LSTM models presented so far for the different problems. This too applies to time series forecasting problems that involve multivariate and multi-step forecasting, but it may be a little more challenging.\n",
    "\n",
    "1. Multiple Input Multi-Step Output.\n",
    "2. Multiple Parallel Input and Multi-Step Output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c862aa1",
   "metadata": {},
   "source": [
    "## 4.1. Multiple Input Multi-step Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e0c74b76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 3, 2) (6, 2)\n"
     ]
    }
   ],
   "source": [
    "## define the split_seq multivariate time series for multiple Input\n",
    "def split_multiple_input_seq(sequences, n_steps_in, n_steps_out):\n",
    "    X, y = list(), list()\n",
    "    for i in range(len(sequences)):\n",
    "        end_ix = i + n_steps_in\n",
    "        out_ix = end_ix + n_steps_out - 1\n",
    "        if out_ix > len(sequences):\n",
    "            break\n",
    "        seq_x, seq_y = sequences[i:end_ix, :-1], sequences[end_ix-1:out_ix, -1]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "## setup input samples\n",
    "n_steps_in, n_steps_out = 3, 2\n",
    "X, y = split_multiple_input_seq(dataset, n_steps_in, n_steps_out)\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2048f32e",
   "metadata": {},
   "source": [
    "- We have 6 samples\n",
    "- The input portion of the samples is 3D, with 3 time steps and 2 variables for 2 input time series\n",
    "- The output portion has 2D, which is 2 time steps for 6 samples "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fcb39f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## define model\n",
    "n_features = X.shape[2]\n",
    "# reuse the encoder-decoder model, demons a vector output with Stacked LSTM\n",
    "def multivariate_model(X, y, n_steps_in, n_steps_out, n_features):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(100, activation='relu', return_sequences=True, input_shape=(n_steps_in, n_features)))\n",
    "    model.add(LSTM(100, activation='relu'))\n",
    "    model.add(Dense(n_steps_out))\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    model.fit(X, y, epochs=500, verbose=0)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b33387e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[185.3084  206.19203]]\n"
     ]
    }
   ],
   "source": [
    "x_input = np.array([[70, 75], [80, 85], [90, 95]])\n",
    "x_input = x_input.reshape((1, n_steps_in, n_features))\n",
    "model = multivariate_model(X, y, n_steps_in, n_steps_out, n_features)\n",
    "yhat = model.predict(x_input, verbose=0)\n",
    "print(yhat)\n",
    "\n",
    "## expected value 185, 205"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a29b97f",
   "metadata": {},
   "source": [
    "## 4.2. Multiple Parallel Input and Multi-step Output\n",
    "A problem with parallel time series may require the prediction of multiple time steps of each time series.\n",
    "\n",
    "We may use the last three time steps from each of the three time series as input to the model and predict the next time steps of each of the three time series as output.\n",
    "\n",
    "Input\n",
    "\n",
    "    10, 15, 25\n",
    "    20, 25, 45\n",
    "    30, 35, 65\n",
    "\n",
    "Output\n",
    "\n",
    "    40, 45, 85\n",
    "    50, 55, 105\n",
    "    \n",
    "We can see that both the input (X) and output (Y) elements of the dataset are three dimensional for the number of samples, time steps, and variables or parallel time series respectively.\n",
    "\n",
    "We can use either the Vector Output or Encoder-Decoder LSTM to model this problem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "511ab655",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 3, 3) (5, 2, 3)\n"
     ]
    }
   ],
   "source": [
    "## multivariate multi-step encoder-decoder lstm example\n",
    "# split data\n",
    "def split_parallel_input_seq(sequences, n_steps_in, n_steps_out):\n",
    "    X, y = list(), list()\n",
    "    for i in range(len(sequences)):\n",
    "        end_ix = i + n_steps_in\n",
    "        out_ix = end_ix + n_steps_out\n",
    "        if out_ix > len(sequences):\n",
    "            break\n",
    "        seq_x, seq_y = sequences[i:end_ix, :], sequences[end_ix:out_ix, :]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "n_steps_in, n_steps_out = 3, 2\n",
    "X, y = split_parallel_input_seq(dataset, n_steps_in, n_steps_out)\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "72565754",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder-decoder model\n",
    "def multiple_parallel_model(X, y, n_steps_in, n_steps_out, n_features):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(200, activation='relu', input_shape=(n_steps_in, n_features)))\n",
    "    model.add(RepeatVector(n_steps_out))\n",
    "    model.add(LSTM(200, activation='relu', return_sequences=True))\n",
    "    model.add(TimeDistributed(Dense(n_features)))\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    model.fit(X, y, epochs=500, verbose=0)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "93fd86c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 90.13544   95.713745 185.54388 ]\n",
      "  [100.09547  106.224075 206.01292 ]]]\n"
     ]
    }
   ],
   "source": [
    "n_features = X.shape[2]\n",
    "x_input = np.array([[60, 65, 125], [70, 75, 145], [80, 85, 165]])\n",
    "x_input = x_input.reshape((1, n_steps_in, n_features))\n",
    "model = multiple_parallel_model(X, y, n_steps_in, n_steps_out, n_features)\n",
    "yhat = model.predict(x_input, verbose=0)\n",
    "print(yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f81a050",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9caaa5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
